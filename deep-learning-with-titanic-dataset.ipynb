{"cells":[{"cell_type":"markdown","metadata":{},"source":["<p style=\"background-color:#90EE90;color:black;font-size:45px;text-align:center;border-radius:9px 9px;font-weight:bold;border:2px black;\">Titanic - SVM and RF Parameters and Deep Learning</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T10:51:14.483658Z","iopub.status.busy":"2022-10-13T10:51:14.483174Z","iopub.status.idle":"2022-10-13T10:51:14.494231Z","shell.execute_reply":"2022-10-13T10:51:14.492495Z","shell.execute_reply.started":"2022-10-13T10:51:14.483558Z"},"trusted":true},"outputs":[],"source":["# Project Planning --\n","# 1. The first thing we want to do is an inspection of the data\n","# 2. Cleaning the data (NaNs, incorrect values, etc)\n","# 3. Transform the data and create dummy variables\n","# 4. The plan is to run both a RandomForest and SVM model\n","# 5. We want the models to be tuned to their best parameters so we will attempt to write a script to do that\n","# 6. We will then see the best feature parameters of both the models to use for deep learning\n","# 7. The deep model will be handled with Keras. We want to first explore the different optimizers (We will run a model using each one and see the different accuracies)\n","# 8. We will then use the best optimizer to create the final fit using K-fold cross-validation over 10 cycles."]},{"cell_type":"markdown","metadata":{},"source":["## <p style=\"background-color:#90EE90;color:black;font-size:30px;text-align:center;border-radius:12px 10px;border:2px;\">   Here is the whole dataset </p>\n","\n","\n","<font size=\"4\">The dataset contains ten categories/variables with 891 passenger details. Survived is the response variable for the study. We are also given a seperate \"test\" set of data that does not give a survival result </font>\n","<p></p>\n","\n","\n","<table style=\"border-collapse: collapse;font-size: 15px; width:800px;\">\n","  <tr>\n","    <th style=\"background-color:#D3DBDD;\">Variable Name </th>\n","    <th style=\"width:500px; background-color:#D3DBDD;\">Description</th>\n","    <th style=\"background-color:#D3DBDD;\">Type</th>\n","  </tr>\n","  <tr>\n","    <td>survival</td>\n","    <td>Did Survive the incident?</td>\n","    <td>Categoricol</td>\n","  </tr>\n","  <tr>\n","    <td>pclass </td>\n","    <td>Class of the ticket</td>\n","    <td>Categoricol</td>\n","  </tr>\n","  <tr>\n","    <td>sex </td>\n","    <td>Gender </td>\n","    <td>Categoricol</td>\n","  </tr>\n","  <tr>\n","    <td>Age </td>\n","    <td>Age of the passenger</td>\n","    <td>Numeric</td>\n","  </tr>\n","  <tr>\n","    <td>sibsp </td>\n","    <td>no of siblings / spouses aboard the Titanic </td>\n","    <td>Numeric</td>\n","  </tr>\n","  <tr>\n","    <td>parch</td>\n","    <td>no of parents / children aboard the Titanic</td>\n","    <td>Numeric</td>\n","  </tr>\n","  <tr>\n","    <td>ticket</td>\n","    <td>Unique ticket number</td>\n","    <td>Categoricol</td>\n","  </tr>\n","  <tr>\n","    <td>fare</td>\n","    <td>Passenger fare </td>\n","    <td>Numeric</td>\n","  </tr>\n","  <tr>\n","    <td>cabin</td>\n","    <td>cabin number </td>\n","    <td>Categoricol</td>\n","  </tr>\n","  <tr>\n","    <td>Embarked</td>\n","    <td>Port of Embarkation</td>\n","    <td>Categoricol</td>\n","  </tr>\n","</table>\n"]},{"cell_type":"markdown","metadata":{},"source":["## <p style=\"background-color:#90EE90;color:black;font-size:30px;text-align:center;border-radius:12px 10px;border:2px;\"> 1. Data Inspection and Exploration </p>\n","\n","<font size=\"4\"> First we are going to do some simple data exploration, getting a feel for the different values used. </font>\n","<p></p>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T10:51:14.496388Z","iopub.status.busy":"2022-10-13T10:51:14.495898Z","iopub.status.idle":"2022-10-13T10:51:15.090984Z","shell.execute_reply":"2022-10-13T10:51:15.089639Z","shell.execute_reply.started":"2022-10-13T10:51:14.496347Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","gender = pd.read_csv('../input/titanic/gender_submission.csv')\n","\n","train_data = pd.read_csv('../input/titanic/train.csv')\n","test_data = pd.read_csv('../input/titanic/test.csv')\n","\n","train_data['train'] = 1 # here we are defining a column so that we can concatenate the test/train data\n","test_data['train'] = 0\n","test_data['Survived'] = np.NaN\n","all_data = pd.concat([train_data,test_data])\n","\n","%matplotlib inline\n","all_data.columns # check the columns"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T10:51:15.094753Z","iopub.status.busy":"2022-10-13T10:51:15.093394Z","iopub.status.idle":"2022-10-13T10:51:15.113609Z","shell.execute_reply":"2022-10-13T10:51:15.112074Z","shell.execute_reply.started":"2022-10-13T10:51:15.094704Z"},"trusted":true},"outputs":[],"source":["# checking for dtype, column name, and seeing missing values\n","train_data.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T10:51:15.119047Z","iopub.status.busy":"2022-10-13T10:51:15.117575Z","iopub.status.idle":"2022-10-13T10:51:15.171898Z","shell.execute_reply":"2022-10-13T10:51:15.170583Z","shell.execute_reply.started":"2022-10-13T10:51:15.118985Z"},"trusted":true},"outputs":[],"source":["# getting the descriptive statistics\n","train_data.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T10:51:15.175091Z","iopub.status.busy":"2022-10-13T10:51:15.174182Z","iopub.status.idle":"2022-10-13T10:51:15.185130Z","shell.execute_reply":"2022-10-13T10:51:15.183339Z","shell.execute_reply.started":"2022-10-13T10:51:15.175022Z"},"trusted":true},"outputs":[],"source":["#splitting data into numeric and categorical\n","dfnum = train_data[['Age','SibSp','Parch','Fare','Survived','Pclass']]\n","dfcat = train_data[['Sex','Ticket','Cabin','Embarked']]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T10:51:15.187358Z","iopub.status.busy":"2022-10-13T10:51:15.186851Z","iopub.status.idle":"2022-10-13T10:51:15.223533Z","shell.execute_reply":"2022-10-13T10:51:15.222138Z","shell.execute_reply.started":"2022-10-13T10:51:15.187303Z"},"trusted":true},"outputs":[],"source":["dfnum.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T10:51:15.226257Z","iopub.status.busy":"2022-10-13T10:51:15.225762Z","iopub.status.idle":"2022-10-13T10:51:15.249553Z","shell.execute_reply":"2022-10-13T10:51:15.248206Z","shell.execute_reply.started":"2022-10-13T10:51:15.226208Z"},"trusted":true},"outputs":[],"source":["dfcat.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T10:51:15.252188Z","iopub.status.busy":"2022-10-13T10:51:15.251393Z","iopub.status.idle":"2022-10-13T10:51:15.597980Z","shell.execute_reply":"2022-10-13T10:51:15.596986Z","shell.execute_reply.started":"2022-10-13T10:51:15.252137Z"},"trusted":true},"outputs":[],"source":["# here is a heatmap of the correlations between the 4 numeric variables\n","# as we can see there is a relationship between parch+sibsp (num of parents + num of siblings)\n","print(dfnum.corr())\n","sns.heatmap(dfnum.corr())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T10:51:15.600162Z","iopub.status.busy":"2022-10-13T10:51:15.599524Z","iopub.status.idle":"2022-10-13T10:51:15.607344Z","shell.execute_reply":"2022-10-13T10:51:15.606489Z","shell.execute_reply.started":"2022-10-13T10:51:15.600124Z"},"trusted":true},"outputs":[],"source":["pd.options.mode.chained_assignment = None  # default='warn'\n","dfnum.Age = dfnum.Age.fillna(dfnum.Age.mean())\n","dfnum.Fare = dfnum.Fare.fillna(dfnum.Fare.median())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T10:51:15.609783Z","iopub.status.busy":"2022-10-13T10:51:15.609121Z","iopub.status.idle":"2022-10-13T10:51:16.748733Z","shell.execute_reply":"2022-10-13T10:51:16.747850Z","shell.execute_reply.started":"2022-10-13T10:51:15.609742Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=[16,12])\n","from scipy import stats\n","\n","\n","plt.subplot(231)\n","sns.histplot(all_data['Age'],kde=True)\n","plt.title('Age')\n","plt.xlabel(f'Shapiro test: {round(stats.shapiro(dfnum[\"Age\"]).pvalue,6)}')\n","\n","plt.subplot(232)\n","sns.histplot(all_data['SibSp'],kde=True)\n","plt.title('SibSp')\n","plt.xlabel(f'Shapiro test: {np.round(stats.shapiro(dfnum[\"SibSp\"]).pvalue,6)}')\n","\n","plt.subplot(233)\n","sns.histplot(all_data['Parch'],kde=True)\n","plt.title('Parch')\n","plt.xlabel(f'Shapiro test: {np.round(stats.shapiro(dfnum[\"Parch\"]).pvalue,6)}')\n","\n","plt.subplot(234)\n","sns.histplot(all_data['Survived'],kde=True)\n","plt.title('Survived')\n","plt.xlabel(f'Shapiro test: {np.round(stats.shapiro(dfnum[\"Survived\"]).pvalue,6)}')\n","\n","plt.subplot(235)\n","sns.histplot(all_data['Pclass'],kde=True)\n","plt.title('Pclass')\n","plt.xlabel(f'Shapiro test: {np.round(stats.shapiro(dfnum[\"Pclass\"]).pvalue,6)}')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T10:51:16.750592Z","iopub.status.busy":"2022-10-13T10:51:16.750043Z","iopub.status.idle":"2022-10-13T10:51:17.609582Z","shell.execute_reply":"2022-10-13T10:51:17.608476Z","shell.execute_reply.started":"2022-10-13T10:51:16.750557Z"},"trusted":true},"outputs":[],"source":["# printing out some stacked histograms and some box plots for age and fare\n","\n","plt.figure(figsize=[16,12])\n","\n","# the fare data looks to be fairly skewed and we know from the shapiro test that it's not parametric\n","plt.subplot(231)\n","plt.boxplot(x=dfnum['Fare'], showmeans = True, meanline = True)\n","plt.title('Fare Boxplot')\n","plt.ylabel('Fare ($)')\n","\n","# the same can be said for the age data - its looking likely that we will use either median or mode to fill in NA values\n","plt.subplot(232)\n","plt.boxplot(dfnum['Age'], showmeans = True, meanline = True)\n","plt.title('Age Boxplot')\n","plt.ylabel('Age (Years)')\n","\n","# just a quick look to see if there is any obvious visual correlation between fare and survival\n","plt.subplot(233)\n","plt.hist(x = [dfnum[dfnum['Survived']==1]['Fare'], dfnum[dfnum['Survived']==0]['Fare']], \n","         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\n","plt.title('Fare Histogram by Survival')\n","plt.xlabel('Fare ($)')\n","plt.ylabel('# of Passengers')\n","plt.legend()\n","\n","# and doing the same for age\n","plt.subplot(234)\n","plt.hist(x = [dfnum[dfnum['Survived']==1]['Age'], dfnum[dfnum['Survived']==0]['Age']], \n","         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\n","plt.title('Age Histogram by Survival')\n","plt.xlabel('Age (Years)')\n","plt.ylabel('# of Passengers')\n","plt.legend()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T10:51:17.611941Z","iopub.status.busy":"2022-10-13T10:51:17.611247Z","iopub.status.idle":"2022-10-13T10:51:17.622559Z","shell.execute_reply":"2022-10-13T10:51:17.621017Z","shell.execute_reply.started":"2022-10-13T10:51:17.611895Z"},"trusted":true},"outputs":[],"source":["dfcat.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T10:51:17.629719Z","iopub.status.busy":"2022-10-13T10:51:17.628867Z","iopub.status.idle":"2022-10-13T10:51:30.893178Z","shell.execute_reply":"2022-10-13T10:51:30.891309Z","shell.execute_reply.started":"2022-10-13T10:51:17.629666Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=[16,12])\n","from scipy import stats\n","\n","\n","plt.subplot(231)\n","sns.barplot(x=dfcat['Sex'].value_counts().index,y=dfcat['Sex'].value_counts()).set(title='Sex')\n","plt.subplot(232)\n","sns.barplot(x=dfcat['Ticket'].value_counts().index,y=dfcat['Ticket'].value_counts()).set(title='Ticket')\n","plt.subplot(233)\n","sns.barplot(x=dfcat['Cabin'].value_counts().index,y=dfcat['Cabin'].value_counts()).set(title='Cabin')\n","plt.subplot(234)\n","sns.barplot(x=dfcat['Embarked'].value_counts().index,y=dfcat['Embarked'].value_counts()).set(title='Embarked')\n","\n","# now looking at some of the categorical data descriptives\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T10:51:30.895803Z","iopub.status.busy":"2022-10-13T10:51:30.894942Z","iopub.status.idle":"2022-10-13T10:51:30.910203Z","shell.execute_reply":"2022-10-13T10:51:30.908137Z","shell.execute_reply.started":"2022-10-13T10:51:30.895760Z"},"trusted":true},"outputs":[],"source":["# noticed that some passengers had multiple rooms, thought it was worth exploring separately\n","train_data['cabin_multiple'] = train_data.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\n","train_data['cabin_multiple'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T10:51:30.913904Z","iopub.status.busy":"2022-10-13T10:51:30.912974Z","iopub.status.idle":"2022-10-13T10:51:31.258785Z","shell.execute_reply":"2022-10-13T10:51:31.257209Z","shell.execute_reply.started":"2022-10-13T10:51:30.913825Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=[16,12])\n","\n","plt.subplot(231)\n","plt.hist(x = [train_data[train_data['Survived']==1]['cabin_multiple'], train_data[train_data['Survived']==0]['cabin_multiple']], \n","         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\n","plt.title('cabin_multiple Histogram by Survival')\n","plt.xlabel('# of Cabins')\n","plt.ylabel('# of Passengers')\n","plt.legend()\n","\n","\n","pd.pivot_table(train_data, index = 'Survived', columns = 'cabin_multiple', values = 'Ticket', aggfunc = 'count')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## <p style=\"background-color:#90EE90;color:black;font-size:30px;text-align:center;border-radius:12px 10px;border:2px;\"> 2. Data Cleaning and Preprocessing </p>\n","\n","\n","<font size=\"4\" color=\"black\"> Now we are getting to the part that will differ for most people. We've inspected the data and found some interesting correlations and nuggets of wisdom. However, choosing which variables to move forward with is an art not a science and every data scientist will do something slightly differently.\n","\n","<font size=\"4\" color=\"black\">I've decided to move forward with the following variables:</font>\n","    \n","*   <font size=\"4\" color=\"black\">'Pclass' --- I've separated the classes into separate variables (P1,P2,P3)</font>\n","*   <font size=\"4\" color=\"black\">'Sex'    --- The sex variable is likely to inform the model a lot based on the correlations</font>\n","*   <font size=\"4\" color=\"black\">'Age'    --- Age seems to be an important variable but I will normalise it and clear all NAs</font>\n","*   <font size=\"4\" color=\"black\">'SibSp'  --- The siblings tell us something about traveling with family which may be helpful</font>\n","*   <font size=\"4\" color=\"black\">'norm_sibsp' --- Here I've normalised the sibling values</font>\n","*   <font size=\"4\" color=\"black\">'Parch' --- Likewise the parentage tells us that people are in groups</font>\n","*   <font size=\"4\" color=\"black\">'Fare' --- Fare tells us the price of the ticket and is therefore indicative of the wealth of the patron</font>\n","*   <font size=\"4\" color=\"black\">'norm_fare' --- I've included the normalised price of ticket with NAs removed</font>\n","*   <font size=\"4\" color=\"black\">'cabin_multiple' --- If the patron had multiple cabins it tells us something about their demographic</font>\n","*   <font size=\"4\" color=\"black\">'numeric_ticket' --- The ticket number may be informative about the location of the cabin</font>\n","*   <font size=\"4\" color=\"black\">'Embarked' --- Perhaps the location they embarked from may relate to their cabin location or wealth</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T10:51:31.260838Z","iopub.status.busy":"2022-10-13T10:51:31.260439Z","iopub.status.idle":"2022-10-13T10:51:31.290736Z","shell.execute_reply":"2022-10-13T10:51:31.289206Z","shell.execute_reply.started":"2022-10-13T10:51:31.260799Z"},"trusted":true},"outputs":[],"source":["\n","# split up the cabin multiple string using quick expression\n","all_data['cabin_multiple'] = all_data.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\n","\n","# split the tickets into number tickets and letter tickets\n","all_data['numeric_ticket'] = all_data.Ticket.apply(lambda x: 1 if x.isnumeric() else 0) # simply binary if they had numbers\n","all_data['ticket_letters'] = all_data.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('/','').lower() if len(x.split(' ')[:-1]) > 0 else 0)\n","\n","# extract name titles from the beginning of names\n","all_data.drop(['Name'],axis=1)\n","#all_data['name_title'] = all_data.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\n","\n","# fill in missing data with median = both age and fare were non-parametric and had very skewed responses so median is more appropriate\n","all_data.Age = all_data.Age.fillna(train_data.Age.median())\n","all_data.Fare = all_data.Fare.fillna(train_data.Fare.median())\n","\n","# remove all the empty values from embarked\n","all_data.dropna(subset=['Embarked'],inplace = False)\n","\n","# normalising the sibling and fare data with a log transformation\n","all_data['norm_sibsp'] = np.log(all_data.SibSp+1)\n","all_data['norm_fare'] = np.log(all_data.Fare+1)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T10:51:31.292634Z","iopub.status.busy":"2022-10-13T10:51:31.292261Z","iopub.status.idle":"2022-10-13T10:51:33.731104Z","shell.execute_reply":"2022-10-13T10:51:33.729802Z","shell.execute_reply.started":"2022-10-13T10:51:31.292601Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=[16,12])\n","\n","sp_norm_data = pd.get_dummies(all_data[['norm_sibsp','SibSp']])\n","plt.subplot(231)\n","sns.histplot(data=sp_norm_data,multiple=\"stack\",kde=True,shrink=1.2)\n","\n","fare_norm_data = pd.get_dummies(all_data[['norm_fare','Fare']])\n","plt.subplot(232)\n","sns.histplot(data=fare_norm_data,multiple=\"stack\",kde=True)\n","\n","sns.histplot()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T10:51:33.734373Z","iopub.status.busy":"2022-10-13T10:51:33.733225Z","iopub.status.idle":"2022-10-13T10:51:35.023610Z","shell.execute_reply":"2022-10-13T10:51:35.022496Z","shell.execute_reply.started":"2022-10-13T10:51:33.734328Z"},"trusted":true},"outputs":[],"source":["# here we are looking at Pclass vs a selection of factors.\n","fig, (axis1,axis2,axis3,axis4) = plt.subplots(1,4,figsize=(18,14))\n","\n","sns.boxplot(x='Pclass',y='Fare',hue='Survived',data=all_data,ax=axis1)\n","axis1.set_title('Pclass vs Fare Survival Comparison')\n","\n","sns.barplot(x='Pclass',y='Survived',hue='Sex',data=all_data,ax=axis2,orient = \"v\")\n","axis2.set_title('Pclass vs Sex Survival Comparison')\n","\n","sns.boxplot(x= 'Pclass',y='SibSp',hue = 'Survived',data=all_data,ax=axis3)\n","axis3.set_title('Pclass vs Siblings Survival Comparison')\n","\n","sns.violinplot(x='Pclass',y='Age',hue='Survived',data=all_data,split=True,ax=axis4)\n","axis4.set_title('Pclass vs Age Survival Comparison')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T10:51:35.025988Z","iopub.status.busy":"2022-10-13T10:51:35.024949Z","iopub.status.idle":"2022-10-13T10:51:36.966563Z","shell.execute_reply":"2022-10-13T10:51:36.965365Z","shell.execute_reply.started":"2022-10-13T10:51:35.025941Z"},"trusted":true},"outputs":[],"source":["#histogram comparison of sex, class, and age by survival\n","sca = sns.FacetGrid(all_data, row = 'Sex', col = 'Pclass', hue = 'Survived')\n","sca.map(plt.hist, 'Age', alpha = .55)\n","sca.add_legend()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T10:51:36.968555Z","iopub.status.busy":"2022-10-13T10:51:36.968119Z","iopub.status.idle":"2022-10-13T10:51:38.126520Z","shell.execute_reply":"2022-10-13T10:51:38.125625Z","shell.execute_reply.started":"2022-10-13T10:51:36.968516Z"},"trusted":true},"outputs":[],"source":["#correlation heatmap of dataset\n","def corr_hm(df):\n","    _ , axis1 = plt.subplots(figsize =(14, 12))\n","    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n","    \n","    _ = sns.heatmap(\n","        df.corr(method = 'spearman'), \n","        cmap = colormap,\n","        square=True, \n","        cbar_kws={'shrink':.9 }, \n","        ax=axis1,\n","        linewidths=0.1,vmax=1.0, linecolor='grey',\n","        annot=True,\n","        annot_kws={'fontsize':14 }\n","    )\n","    \n","    plt.title('Spearman Correlation of Features', y=1.05, size=15)\n","\n","corr_hm(all_data)"]},{"cell_type":"markdown","metadata":{},"source":["## <p style=\"background-color:#90EE90;color:black;font-size:30px;text-align:center;border-radius:12px 10px;border:2px;\"> 3. Transforming the Data and Creating Dummies </p>\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T10:51:38.128768Z","iopub.status.busy":"2022-10-13T10:51:38.127640Z","iopub.status.idle":"2022-10-13T10:51:38.151769Z","shell.execute_reply":"2022-10-13T10:51:38.150540Z","shell.execute_reply.started":"2022-10-13T10:51:38.128729Z"},"trusted":true},"outputs":[],"source":["# transforming the class into a string\n","all_data.Pclass = all_data.Pclass.astype(str)\n","\n","# create dummies for analysis\n","all_dummies = pd.get_dummies(all_data[['Pclass','Sex','Age','SibSp','Fare','norm_sibsp','norm_fare','Parch','Embarked',\n","                                     'cabin_multiple','numeric_ticket','train','Survived']])\n","\n","# separate the data back into training and testing sets\n","X_train = all_dummies[all_dummies.train == 1].drop(['train','Survived'], axis = 1)\n","X_test = all_dummies[all_dummies.train == 0].drop(['train'], axis = 1)\n","\n","y_train = all_data[all_data.train==1].Survived\n","y_train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T10:51:38.153457Z","iopub.status.busy":"2022-10-13T10:51:38.153090Z","iopub.status.idle":"2022-10-13T10:51:38.212740Z","shell.execute_reply":"2022-10-13T10:51:38.211197Z","shell.execute_reply.started":"2022-10-13T10:51:38.153400Z"},"trusted":true},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","scale = StandardScaler()\n","\n","# now we are transforming the values into something scaled using StandardScaler from sklearn\n","all_dummies_scaled = all_dummies.copy()\n","all_dummies_scaled[['Age','SibSp','Parch','norm_fare']] = scale.fit_transform(all_dummies_scaled[['Age','SibSp','Parch','norm_fare']])\n","\n","# we are then separating the training and testing sets out with the scaled data\n","X_train_scaled = all_dummies_scaled[all_dummies_scaled.train == 1].drop(['train','Survived'], axis = 1)\n","X_test_scaled = all_dummies_scaled[all_dummies_scaled.train == 0].drop(['train'], axis = 1)\n","\n","y_train = all_data[all_data.train == 1].Survived"]},{"cell_type":"markdown","metadata":{},"source":["## <p style=\"background-color:#90EE90;color:black;font-size:30px;text-align:center;border-radius:12px 10px;border:2px;\"> 4. Building the Model </p>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T10:51:38.216557Z","iopub.status.busy":"2022-10-13T10:51:38.216017Z","iopub.status.idle":"2022-10-13T10:51:38.290025Z","shell.execute_reply":"2022-10-13T10:51:38.289032Z","shell.execute_reply.started":"2022-10-13T10:51:38.216509Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import cross_val_score\n","from sklearn import tree\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import SVC\n","from sklearn.inspection import permutation_importance\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"markdown","metadata":{},"source":["<font size=\"6\" color=\"black\"> Model Tuning\n","\n","<font size=\"4\" color=\"black\"> For the models we are running a Random Forest Classifier and a Support Vector Machine.\n","    I have run a GridSearch with different tuning parameters to identify the best setup to use for these classifiers.\n","    For Random Forest that was the following:\n","    \n"," <font size=\"4\" color=\"black\">   \n","     \n","    bootstrap: True\n","     \n","    criterion: gini\n","     \n","    max_depth: 15\n","     \n","    max_features: 10\n","     \n","    min_samples_leaf: 3\n","     \n","    min_samples_split: 2\n","     \n","    n_estimators: 550\n","     \n","\n","</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T10:51:38.291851Z","iopub.status.busy":"2022-10-13T10:51:38.291494Z","iopub.status.idle":"2022-10-13T10:51:39.709595Z","shell.execute_reply":"2022-10-13T10:51:39.708489Z","shell.execute_reply.started":"2022-10-13T10:51:38.291819Z"},"trusted":true},"outputs":[],"source":["rf = RandomForestClassifier(random_state = 1,bootstrap=True,criterion='gini',max_depth=15,max_features=10,min_samples_leaf=3,min_samples_split=2,\n","                           n_estimators=550)\n","best_clf_rf = rf.fit(X_train_scaled,y_train)"]},{"cell_type":"markdown","metadata":{},"source":["<font size=\"4\" color=\"black\"> I performed the same GridSearch for the SVM and found the following best parameters:\n","    \n"," <font size=\"4\" color=\"black\">   \n","     \n","    C: 1\n","     \n","    gamma: 0.1\n","     \n","    kernel: rbf\n","     "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T10:51:39.712368Z","iopub.status.busy":"2022-10-13T10:51:39.711157Z","iopub.status.idle":"2022-10-13T10:51:39.897285Z","shell.execute_reply":"2022-10-13T10:51:39.896003Z","shell.execute_reply.started":"2022-10-13T10:51:39.712313Z"},"trusted":true},"outputs":[],"source":["svc = SVC(probability = True,C=1.0,kernel=\"rbf\",gamma=0.1)\n","best_clf_svc = svc.fit(X_train_scaled,y_train)"]},{"cell_type":"markdown","metadata":{},"source":["<font size=\"4\" color=\"black\"> Here we are extracting the feature importance for the two models. The plan is to take the combined top10 features of both models to use as our features for the deep learning.\n","    \n"," <font size=\"4\" color=\"black\">  "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T10:51:39.899278Z","iopub.status.busy":"2022-10-13T10:51:39.898784Z","iopub.status.idle":"2022-10-13T10:51:51.837629Z","shell.execute_reply":"2022-10-13T10:51:51.836240Z","shell.execute_reply.started":"2022-10-13T10:51:39.899209Z"},"trusted":true},"outputs":[],"source":["\n","perm_importance = permutation_importance(best_clf_rf, X_train_scaled, y_train)\n","feat_importances_rf = pd.Series(perm_importance.importances_mean,index=X_train_scaled.columns)\n","\n","feat_importances_rf_plt = feat_importances_rf.nlargest(10).plot(kind='barh')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T10:51:51.839889Z","iopub.status.busy":"2022-10-13T10:51:51.839349Z","iopub.status.idle":"2022-10-13T10:51:55.198706Z","shell.execute_reply":"2022-10-13T10:51:55.197355Z","shell.execute_reply.started":"2022-10-13T10:51:51.839833Z"},"trusted":true},"outputs":[],"source":["perm_importance = permutation_importance(best_clf_svc, X_train_scaled, y_train)\n","feat_importances_svm = pd.Series(perm_importance.importances_mean,index=X_train_scaled.columns)\n","\n","feat_importances_svm_plt = feat_importances_svm.nlargest(10).plot(kind='barh')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T10:51:55.201448Z","iopub.status.busy":"2022-10-13T10:51:55.200950Z","iopub.status.idle":"2022-10-13T10:51:55.217865Z","shell.execute_reply":"2022-10-13T10:51:55.215948Z","shell.execute_reply.started":"2022-10-13T10:51:55.201392Z"},"trusted":true},"outputs":[],"source":["# find the common features for deep learning\n","a=feat_importances_svm.nlargest(10).index\n","b=feat_importances_rf.nlargest(10).index\n","df=pd.DataFrame([a,b]).transpose()#\n","\n","df = df[0].append(df[1]).drop_duplicates('first')\n","\n","X_train_DL = X_train_scaled[df]\n","X_test_DL = X_test_scaled[df]\n","\n","y_train_DL = y_train\n","\n","dense_neuron = X_train_DL.shape[1]"]},{"cell_type":"markdown","metadata":{},"source":["<font size=\"4\" color=\"black\"> I ran the deep learning model looping through the 8 different optimizers to find the version with the highest accuracy/lowest loss\n","    \n","    \n","<table style=\"border-collapse: collapse;font-size: 15px; width:800px;\">\n","  <tr>\n","    <th style=\"background-color:#D3DBDD;\">Optimizer </th>\n","    <th style=\"background-color:#D3DBDD;\">Loss</th>\n","    <th style=\"background-color:#D3DBDD;\">Accuracy Percentage</th>\n","  </tr>\n","  <tr>\n","    <td>SGD</td>\n","    <td>0.125</td>\n","    <td>83.38%</td>\n","  </tr>\n","  <tr>\n","      <td><b>RMSprop</b> </td>\n","    <td><b>0.0976</b></td>\n","    <td><b>87.54%</b></td>\n","  </tr>\n","  <tr>\n","    <td>Adam </td>\n","    <td>0.108 </td>\n","    <td>86.083%</td>\n","  </tr>\n","  <tr>\n","    <td>Adadelta </td>\n","    <td>0.264</td>\n","    <td>37.59%</td>\n","  </tr>\n","  <tr>\n","    <td>Adagrad </td>\n","    <td>0.133 </td>\n","    <td>82.37%</td>\n","  </tr>\n","  <tr>\n","    <td>Adamax</td>\n","    <td>0.110</td>\n","    <td>85.29%</td>\n","  </tr>\n","  <tr>\n","    <td>Nadam</td>\n","    <td>0.106</td>\n","    <td>86.08%</td>\n","  </tr>\n","  <tr>\n","    <td>Ftrl</td>\n","    <td>0.249 </td>\n","    <td>61.61%</td>\n","  </tr>\n","</table>\n"," \n","<font size=\"4\" color=\"black\"> As we can see, RMRprop gave the best results, so we will use that optimizer moving forward"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T10:51:55.220337Z","iopub.status.busy":"2022-10-13T10:51:55.219873Z","iopub.status.idle":"2022-10-13T10:51:55.232576Z","shell.execute_reply":"2022-10-13T10:51:55.230739Z","shell.execute_reply.started":"2022-10-13T10:51:55.220300Z"},"trusted":true},"outputs":[],"source":["# separating the training data into train/test splits for use in our model fitting\n","XDL_train,XDL_test,yDL_train,yDL_test=train_test_split(X_train_DL,y_train,test_size=0.2,random_state=1234)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T12:07:28.333630Z","iopub.status.busy":"2022-10-13T12:07:28.333058Z","iopub.status.idle":"2022-10-13T12:07:28.344648Z","shell.execute_reply":"2022-10-13T12:07:28.343382Z","shell.execute_reply.started":"2022-10-13T12:07:28.333576Z"},"trusted":true},"outputs":[],"source":["# here we are defining some parameters to use in our model\n","\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from sklearn.model_selection import KFold\n","from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n","from tqdm.notebook import tqdm\n","from sklearn.utils import shuffle\n","import tensorflow.keras.backend as K\n","\n","COLS = list(XDL_train.columns) # defining the columns\n","\n","batch_size=128\n","epoch = 500\n","mae = pd.DataFrame\n","kFold = KFold(n_splits=10,\n","              shuffle=True) # defining the number of cross-validation kfolds we want to perform\n","\n","inputs = np.concatenate((XDL_train,\n","                         XDL_test),\n","                        axis=0) # combining the test/train data\n","\n","targets = np.concatenate((yDL_train,\n","                          yDL_test),\n","                         axis=0)"]},{"cell_type":"markdown","metadata":{},"source":["<font size=\"4\" color=\"black\"> Brace yourself, this is a wall of text. I could have subdivided this into several functions but it all works and runs well. Perhaps I can clean it up a bit another time. But for now lets consider what this model is achieving:\n","    \n","* <font size=\"3\" color=\"black\">First we are defining a table that will output our mean absolute error scores.\n","    \n","* <font size=\"3\" color=\"black\">Next we are collecting the results into lists that we can use for printing.\n","    \n","* <font size=\"3\" color=\"black\">The model then enters into an outer for loop that will loop over the number of folds that we want to run (defined above).\n","    \n","* <font size=\"3\" color=\"black\">Within this loop we will re-define the model on every run, compile the model using our predefined standards and fit the model.\n","    \n","* <font size=\"3\" color=\"black\">Before we fit, we are creating several callbacks that can be used during the run:\n","    \n","   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;        1.<font size=\"3\" color=\"black\"> A callback that will stop the current run if certain amount of progression has passed without improvement.\n","       \n","  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;         2.<font size=\"3\" color=\"black\"> A callback that saves the best weights based on improvements across each epoch.\n","      \n","* <font size=\"3\" color=\"black\">The data is split manually based on the kFolds function defined above. Each iteration of the outer loop will use a different subsection of the data for test and validation.\n","    \n","* <font size=\"3\" color=\"black\">After the data are fit to the model, we want to extract the feature importance much like we have done above.\n","    \n","* <font size=\"3\" color=\"black\">For this to work first we must define a baseline score, here calculated by defining the out of function predictions based on all the valid data and then taking the absolute mean of that score divided by the valid list.\n","    \n","* <font size=\"3\" color=\"black\">After this we enter into an inner loop which does the same thing for each feature but shuffles the column each time.\n","    \n","* <font size=\"3\" color=\"black\">The last part of the function is just dealing with the outputs </font>   "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T12:10:32.793196Z","iopub.status.busy":"2022-10-13T12:10:32.792649Z","iopub.status.idle":"2022-10-13T12:10:32.825887Z","shell.execute_reply":"2022-10-13T12:10:32.824676Z","shell.execute_reply.started":"2022-10-13T12:10:32.793155Z"},"trusted":true},"outputs":[],"source":["def DLmodel(inputs,targets,columns,batch_size,epochs,kFold):\n","    results_table = {}\n","    acc_per_fold = [] # variable to collect the accuracy for printing and saving\n","    loss_per_fold = [] # variable to collect the loss for printing and saving\n","    i = 0\n","    fold_no = 1\n","    for fold,(train_idx,test_idx) in enumerate(kFold.split(inputs,targets)):\n","    \n","        K.clear_session()\n","        \n","        print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n","        X_train, X_valid = inputs[train_idx], inputs[test_idx]\n","        y_train, y_valid = targets[train_idx], targets[test_idx]\n","        checkpoint_filepath = f\"folds{fold}.hdf5\"\n","    \n","        #################################################################\n","        # creating the keras model\n","        model = keras.Sequential([\n","            layers.BatchNormalization(),\n","            layers.Dense(32,\n","                         activation = 'relu',\n","                         input_shape=([dense_neuron])),\n","            layers.Dropout(0.3),\n","            layers.BatchNormalization(),\n","            layers.Dense(32,\n","                         activation = 'relu'),\n","            layers.BatchNormalization(),\n","            layers.Dense(32,\n","                         activation = 'relu'),\n","            layers.Dropout(0.3),\n","            layers.Dense(1,\n","                         activation = 'sigmoid')\n","        ])\n","\n","        model.compile(\n","            optimizer = 'RMSprop',\n","            loss = 'mae',\n","            metrics=['accuracy']\n","    \n","        )\n","        #################################################################\n","        # Generate a print\n","        print('------------------------------------------------------------------------')\n","        print(f'Training for fold {fold_no} ...')\n","    \n","        #################################################################\n","        # create the early stopping and saving functionality\n","        early_stopping = keras.callbacks.EarlyStopping(\n","            patience = 100,\n","            min_delta = 0.001,\n","            restore_best_weights=True,\n","        )\n","        save_best = keras.callbacks.ModelCheckpoint(\n","            filepath=checkpoint_filepath,\n","            save_weights_only=False,\n","            monitor='val_accuracy',\n","            mode='max',\n","            verbose=0,\n","            save_best_only=True)\n","\n","        # fit the model\n","        history = model.fit(\n","            X_train,\n","            y_train,\n","            validation_data=(X_valid,\n","                             y_valid),    \n","            batch_size=batch_size,\n","            epochs = epochs,\n","            callbacks=[early_stopping,\n","                       save_best],\n","            verbose = False,\n","        )\n","        scores = model.evaluate(X_valid,\n","                                y_valid,\n","                                verbose=0)\n","        print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]};{model.metrics_names[1]} of {scores[1]*100}%')\n","        print(f'optimizer: RMSprop')\n","        acc_per_fold.append(scores[1]*100)\n","        loss_per_fold.append(scores[0])   \n","\n","        #################################################################\n","        # Increase fold number\n","\n","        \n","        print('Calculating feature importance...')\n","            \n","        # compute baseline (no shuffle)\n","        oof_preds = model.predict(X_valid,\n","                                  verbose=0).squeeze() \n","        baseline_mae = np.mean(np.abs(oof_preds-y_valid ))\n","    \n","        for col in columns:\n","            if not col in results_table.keys():\n","                results_table[col] = []\n","                results_table['baseline'] = []\n","       \n","        results_table['baseline'].append(baseline_mae)\n","    \n","        ################################################################# \n","        # add col names before loops\n","    \n","\n","        # defining a function to take care of the inner loop calculations\n","        for k in tqdm(range(len(columns))):\n","            # shuffle feature \n","            save_col = X_valid[:,k].copy()\n","            np.random.shuffle(X_valid[:,k])\n","            col_name = columns[k]\n","            # computer oof mae with feature k shuffled\n","            oof_preds = model.predict(X_valid,\n","                                        verbose=0).squeeze()\n","\n","            results_table[col_name].append(np.mean(np.abs(oof_preds-y_valid)))\n","            X_valid[:,k] = save_col\n","    \n","        \n","    \n","        #################################################################\n","        # display feature importance\n","        \n","        df = pd.DataFrame.from_dict(results_table)\n","        df = df.iloc[fold].sort_values(ascending=True)\n","        plt.figure(figsize=(5,6))\n","        plt.barh(np.arange(len(columns)+1),\n","                 df.values)\n","        plt.yticks(np.arange(len(columns)+1),\n","                   df.index)\n","        plt.title(f'K-Fold {fold+1} Feature Importance',\n","                  size=16)\n","        plt.ylim((-1,len(columns)+1))\n","        plt.plot([baseline_mae,\n","                  baseline_mae],\n","                 [-1,len(columns)+1],\n","                 '--', color='red',\n","                 label=f'Baseline OOF\\nMAE={baseline_mae:.3f}')\n","        plt.xlabel(f'Fold {fold+1} OOF MAE with feature permuted',\n","                   size=14)\n","        plt.ylabel('Feature',\n","                   size=14)\n","        plt.legend()\n","        plt.show()\n","                               \n","        # SAVE FEATURE IMPORTANCE\n","        df.to_csv(f'feature_importance_fold_{fold+1}.csv',\n","                  index=False)\n","    \n","        fold_no = fold_no + 1\n","        i = i+1\n","    \n","    #################################################################\n","\n","    # == Provide average scores ==\n","    print('------------------------------------------------------------------------')\n","    print('Score per fold')\n","    for i in range(0, len(acc_per_fold)):\n","      print('------------------------------------------------------------------------')\n","      print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n","    print('------------------------------------------------------------------------')\n","    print('Average scores for all folds:')\n","    print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n","    print(f'> Loss: {np.mean(loss_per_fold)}')\n","    print('------------------------------------------------------------------------')\n","\n","    history_df = pd.DataFrame(history.history)\n","    history_df.loc[:, ['loss', 'val_loss','accuracy','val_accuracy']].plot();\n","    print(\"Minimum validation loss: {}\".format(history_df['val_loss'].min()))\n","    pd.DataFrame.from_dict(results_table)\n","    \n","    return history, results_table, save_best"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T12:10:33.765738Z","iopub.status.busy":"2022-10-13T12:10:33.765283Z","iopub.status.idle":"2022-10-13T12:14:03.030104Z","shell.execute_reply":"2022-10-13T12:14:03.028721Z","shell.execute_reply.started":"2022-10-13T12:10:33.765702Z"},"trusted":true},"outputs":[],"source":["history,results_table, save_best = DLmodel(inputs,targets,COLS,batch_size,epoch,kFold)"]},{"cell_type":"markdown","metadata":{},"source":["<font size=\"4\" color=\"black\"> As we can see from the output, the model is performing pretty well (between 80-90% accuracy). The plan now, which is totally not necessary and is purely for exploration at this point. Is to take the overall best features across all of the folds and take the best 5 and apply it to one more run. Just to see if using less features that are the most informative is able to squeeze out a little more accuracy. Obviously we want to be careful as well to keep an eye on our loss/val_loss scores to make sure we aren't overfitting/underfitting."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T12:14:08.053900Z","iopub.status.busy":"2022-10-13T12:14:08.052530Z","iopub.status.idle":"2022-10-13T12:14:08.334968Z","shell.execute_reply":"2022-10-13T12:14:08.333496Z","shell.execute_reply.started":"2022-10-13T12:14:08.053827Z"},"trusted":true},"outputs":[],"source":["from matplotlib.pyplot import figure\n","\n","param_data = pd.DataFrame.from_dict(results_table)\n","mean_param_data = {}\n","for i in param_data:\n","    mean_param_data[i] = []\n","    mean_param_data[i].append(param_data[i].mean())\n","figure(figsize=(16, 12), dpi=80)\n","plt.scatter(*zip(*sorted(mean_param_data.items()))) \n","plt.show()\n","mean_param_data = pd.Series(mean_param_data)"]},{"cell_type":"markdown","metadata":{},"source":["<font size=\"3\" color=\"black\"> Here is a simple scatter plot detailing the average feature importance across all the folds. We are going to take the best 5 and use it for the final model."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T12:14:13.550109Z","iopub.status.busy":"2022-10-13T12:14:13.549194Z","iopub.status.idle":"2022-10-13T12:14:13.562288Z","shell.execute_reply":"2022-10-13T12:14:13.559929Z","shell.execute_reply.started":"2022-10-13T12:14:13.550056Z"},"trusted":true},"outputs":[],"source":["top5_params = mean_param_data.sort_values(ascending = False)[0:5]\n","top5COLS = list(top5_params.index)\n","\n","inputs = np.concatenate((XDL_train[top5COLS],\n","                         XDL_test[top5COLS]),\n","                        axis=0) # combining the test/train data\n","\n","targets = np.concatenate((yDL_train,\n","                          yDL_test),\n","                         axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T12:14:22.820807Z","iopub.status.busy":"2022-10-13T12:14:22.820187Z","iopub.status.idle":"2022-10-13T12:17:31.935385Z","shell.execute_reply":"2022-10-13T12:17:31.933924Z","shell.execute_reply.started":"2022-10-13T12:14:22.820761Z"},"trusted":true},"outputs":[],"source":["best_params,results_table_params, save_best_params = DLmodel(inputs,targets,top5COLS,batch_size,epoch,kFold)"]},{"cell_type":"markdown","metadata":{},"source":["<font size=\"3\" color=\"black\"> Accuracy seems to be staying put, still around 80-90%. Which is interesting in of itself. Even without the other variables we are still able to predict survival likelihood to a very high percentage."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T13:14:35.981140Z","iopub.status.busy":"2022-10-13T13:14:35.980647Z","iopub.status.idle":"2022-10-13T13:14:37.588415Z","shell.execute_reply":"2022-10-13T13:14:37.587320Z","shell.execute_reply.started":"2022-10-13T13:14:35.981102Z"},"trusted":true},"outputs":[],"source":["Xfinal_train = X_train_DL[top5_params.index] # creating the testing set with the finalised features\n","Xfinal_test = X_test_DL[top5_params.index]\n","best_final = {}\n","for i in range(kFold.n_splits): \n","    # this loop extracts the accuracy and loss from each of our folds evaluated on the whole.\n","    # training set rather than using out of sample kFolds. We will then save this into a library and find the very best weights to use.\n","    # this isn't strictly best practice, but like I said earlier, this is more just for exploration and seeing what we can squeeze out.\n","    save_best_params.model.load_weights(f'folds{i}.hdf5')\n","    best_final[f'folds{i}.hdf5'] = []\n","    best_final[f'folds{i}.hdf5'].append(save_best_params.model.evaluate(Xfinal_train,y_train_DL,verbose=True)[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T13:14:54.536123Z","iopub.status.busy":"2022-10-13T13:14:54.535537Z","iopub.status.idle":"2022-10-13T13:14:54.573679Z","shell.execute_reply":"2022-10-13T13:14:54.571678Z","shell.execute_reply.started":"2022-10-13T13:14:54.536079Z"},"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T13:15:40.827214Z","iopub.status.busy":"2022-10-13T13:15:40.826743Z","iopub.status.idle":"2022-10-13T13:15:40.993782Z","shell.execute_reply":"2022-10-13T13:15:40.992584Z","shell.execute_reply.started":"2022-10-13T13:15:40.827178Z"},"trusted":true},"outputs":[],"source":["save_best_params.model.load_weights(max(best_final))\n","save_best_params.model.evaluate(Xfinal_train,y_train_DL,verbose=True)"]},{"cell_type":"markdown","metadata":{},"source":["## <p style=\"background-color:#90EE90;color:black;font-size:30px;text-align:center;border-radius:12px 10px;border:2px;\"> 4. Model Prediction of Survival </p>\n","\n","<font size=\"3\" color=\"black\"> So here we are at the moment of truth! We are going to use the testing data with our modified feature inputs to predict survival and submit to the competition!\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from keras import models\n","\n","print('Generate a prediction')\n","prediction = np.round(save_best_params.model.predict(Xfinal_test)).astype(int)\n","\n","print('prediction shape:',prediction.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["final_data = {'PassengerId':test_data.PassengerId,\n","             'Survived':prediction.transpose(1,0)[0,:]}\n","\n","submission = pd.DataFrame(final_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["submission.to_csv('submission.csv',index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.5 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"},"vscode":{"interpreter":{"hash":"f42f955e7f8da3c548e302e1e5347a7e875359e0d4da102c4592ccaca30f562a"}}},"nbformat":4,"nbformat_minor":4}
